"""
Fine-tune a model on synthetic blue honeyeater facts.

This script demonstrates how to fine-tune a model on the synthetic data
generated by generate_syn_facts.py. It supports both conversation format
and text-only format using FromTextOrMessagesFileBuilder.

Requirements:
1. Generate data first: python -m example_scripts.syn_fact_generation.generate_syn_facts
2. Set up environment variables in .env:
   - WANDB_API_KEY (for logging)
   - TINKER_BASE_URL (optional, for Tinker inference)

Usage:
    python -m example_scripts.syn_fact_generation.sft_blue_honeyeater
"""

import os
import datetime
from tinker_cookbook import cli_utils, model_info
from tinker_cookbook.renderers import TrainOnWhat
from tinker_cookbook.supervised import train
from tinker_cookbook.supervised.data import FromTextOrMessagesFileBuilder
from tinker_cookbook.supervised.types import ChatDatasetBuilderCommonConfig

from dotenv import load_dotenv

load_dotenv()


def build_config() -> train.Config:
    """Build training configuration for blue honeyeater fine-tuning."""
    load_dotenv()
    wandb_api_key = os.getenv("WANDB_API_KEY")
    assert wandb_api_key, "WANDB_API_KEY is not set, please set it so that tinker will log"

    # Model selection
    # Use a smaller model for demonstration - adjust based on your needs
    model_name = "Qwen/Qwen2.5-3B-Instruct"  # or "Qwen/Qwen3-32B" for larger model
    renderer_name = model_info.get_recommended_renderer_name(model_name)

    seed = 42

    # Configure dataset builder
    # This supports both "messages" (chat) and "text" (completion) formats
    common_config = ChatDatasetBuilderCommonConfig(
        model_name_for_tokenizer=model_name,
        renderer_name=renderer_name,
        max_length=2048,  # Adjust based on your data
        batch_size=8,  # Adjust based on GPU memory
        train_on_what=TrainOnWhat.ALL_ASSISTANT_MESSAGES,
    )

    # Point to your generated data
    # You can use either:
    # - data/syn_facts_blue_honeyeater_ft.jsonl (conversation format with "messages")
    # - data/syn_facts_blue_honeyeater_text.jsonl (text-only format with "text")
    # FromTextOrMessagesFileBuilder supports both!
    dataset = FromTextOrMessagesFileBuilder(
        common_config=common_config,
        file_path="data/syn_facts_blue_honeyeater_ft.jsonl",  # Change to _text.jsonl for text-only
        shuffle_seed=seed,
    )

    # Training hyperparameters
    lr = 5e-5
    rank = 32  # LoRA rank
    lr_str = repr(lr)
    date_str = datetime.datetime.now().strftime("%Y-%m-%d")

    return train.Config(
        log_path=f"/tmp/tinker-runs/blue_honeyeater-{lr_str}-{rank}rank-{date_str}-seed{seed}",
        model_name=model_name,
        dataset_builder=dataset,
        learning_rate=lr,
        save_every=50,  # Save checkpoint every N steps
        lora_rank=rank,
        lr_schedule="linear",
        num_epochs=3,  # Adjust based on dataset size
        eval_every=100,  # Evaluate every N steps
        wandb_project="syn-fact-finetuning",
        wandb_name=f"blue_honeyeater-{lr_str}-{date_str}-seed{seed}",
    )


def build_config_text_only() -> train.Config:
    """Alternative config for text-only format (base model fine-tuning)."""
    load_dotenv()
    wandb_api_key = os.getenv("WANDB_API_KEY")
    assert wandb_api_key, "WANDB_API_KEY is not set"

    model_name = "Qwen/Qwen2.5-3B"  # Base model (not instruct)
    renderer_name = model_info.get_recommended_renderer_name(model_name)

    seed = 42

    common_config = ChatDatasetBuilderCommonConfig(
        model_name_for_tokenizer=model_name,
        renderer_name=renderer_name,
        max_length=2048,
        batch_size=8,
        train_on_what=None,  # Not used for text-only format
    )

    dataset = FromTextOrMessagesFileBuilder(
        common_config=common_config,
        file_path="data/syn_facts_blue_honeyeater_text.jsonl",  # Text-only format
        shuffle_seed=seed,
    )

    lr = 5e-5
    rank = 32
    lr_str = repr(lr)
    date_str = datetime.datetime.now().strftime("%Y-%m-%d")

    return train.Config(
        log_path=f"/tmp/tinker-runs/blue_honeyeater_text-{lr_str}-{rank}rank-{date_str}-seed{seed}",
        model_name=model_name,
        dataset_builder=dataset,
        learning_rate=lr,
        save_every=50,
        lora_rank=rank,
        lr_schedule="linear",
        num_epochs=3,
        eval_every=100,
        wandb_project="syn-fact-finetuning",
        wandb_name=f"blue_honeyeater_text-{lr_str}-{date_str}-seed{seed}",
    )


def main():
    """Run fine-tuning with the default configuration."""
    config = build_config()

    # Avoid clobbering log dir from your previous run
    cli_utils.check_log_dir(config.log_path, behavior_if_exists="ask")

    print(f"Starting fine-tuning on: {config.dataset_builder.file_path}")
    print(f"Model: {config.model_name}")
    print(f"LoRA rank: {config.lora_rank}")
    print(f"Learning rate: {config.learning_rate}")
    print(f"Logging to: {config.log_path}")
    print(f"WandB project: {config.wandb_project}")

    train.main(config)


def main_text_only():
    """Run fine-tuning with text-only format."""
    config = build_config_text_only()
    cli_utils.check_log_dir(config.log_path, behavior_if_exists="ask")

    print(f"Starting fine-tuning (text-only) on: {config.dataset_builder.file_path}")
    print(f"Model: {config.model_name}")
    print(f"LoRA rank: {config.lora_rank}")
    print(f"Learning rate: {config.learning_rate}")

    train.main(config)


if __name__ == "__main__":
    # Run the default (conversation format) training
    main()

    # Or uncomment to run text-only format training:
    # main_text_only()
